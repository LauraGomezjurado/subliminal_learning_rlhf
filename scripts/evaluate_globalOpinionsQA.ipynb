{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBcDkXzzcHE5",
        "outputId": "46d842ed-5c62-4a70-f552-9f18b62c9f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "TWO-MODEL COMPARISON ON GLOBALOPINIONSQA\n",
            "Metrics: JS Similarity & Agreement Rate\n",
            "================================================================================\n",
            "\n",
            "‚úì Configuration:\n",
            "  Model A: UK (uk)\n",
            "  Model B: Mexico (mexico)\n",
            "  Will evaluate on: United Kingdom and Mexico\n",
            "\n",
            "  Expected zip files:\n",
            "    - uk_dpo_model.zip\n",
            "    - mexico_dpo_model.zip\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Compare Two Models on GlobalOpinionsQA\n",
        "\n",
        "========================================\n",
        "SIMPLIFIED WORKFLOW - JUST SET 2 COUNTRIES!\n",
        "========================================\n",
        "\n",
        "‚≠ê QUICK START:\n",
        "   1. Go to CELL 1b (line ~50)\n",
        "   2. Change these two lines:\n",
        "      COUNTRY_A = Country.MEXICO  # Change to: US, UK, CHILE, or MEXICO\n",
        "      COUNTRY_B = Country.CHILE   # Change to: US, UK, CHILE, or MEXICO\n",
        "   3. Done! Everything else is automatic.\n",
        "\n",
        "Metrics Computed:\n",
        "    ‚Ä¢ JS Similarity: Overall distributional alignment (1 - Jensen-Shannon Distance)\n",
        "    ‚Ä¢ Agreement Rate: How often model's argmax matches human majority choice\n",
        "\n",
        "Usage:\n",
        "\n",
        "    LOCAL:\n",
        "        1. Set COUNTRY_A and COUNTRY_B in CELL 1b (options: US, UK, CHILE, MEXICO)\n",
        "        2. Extract model zips to ./models/<country>/\n",
        "        3. Run: python scripts/evaluate_two_models.py\n",
        "\n",
        "    GOOGLE COLAB:\n",
        "        1. Upload this file to Colab\n",
        "        2. Edit CELL 1b: Set COUNTRY_A and COUNTRY_B (e.g., Country.MEXICO, Country.CHILE)\n",
        "        3. Run CELL 1 (imports + config)\n",
        "        4. Run CELL 2 and upload your two model zip files\n",
        "           - Expected: <country>_dpo_model.zip (e.g., mexico_dpo_model.zip)\n",
        "           - Created in training with: shutil.make_archive(\"mexico_dpo_model\", 'zip', \"results/dpo_models/mexico/final\")\n",
        "        5. Run remaining cells in order - everything auto-configured!\n",
        "\n",
        "    The script automatically:\n",
        "        ‚úì Sets model paths based on country names\n",
        "        ‚úì Sets evaluation countries (evaluates on both countries' ground truth)\n",
        "        ‚úì Names models appropriately\n",
        "        ‚úì Saves results to <country_a>_vs_<country_b>_comparison.json\n",
        "\n",
        "    You only need to change ONE thing: COUNTRY_A and COUNTRY_B in CELL 1b!\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 1: Imports and Setup\n",
        "# ============================================================================\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import ast\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from scipy.special import softmax\n",
        "from scipy import stats\n",
        "from enum import Enum\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TWO-MODEL COMPARISON ON GLOBALOPINIONSQA\")\n",
        "print(\"Metrics: JS Similarity & Agreement Rate\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 1b: Country Configuration (EDIT THIS!)\n",
        "# ============================================================================\n",
        "class Country(Enum):\n",
        "    \"\"\"Valid countries for model comparison.\"\"\"\n",
        "    US = \"us\"\n",
        "    UK = \"uk\"\n",
        "    CHILE = \"chile\"\n",
        "    MEXICO = \"mexico\"\n",
        "\n",
        "    @property\n",
        "    def display_name(self):\n",
        "        \"\"\"Get display name for the country.\"\"\"\n",
        "        return {\n",
        "            Country.US: \"United States\",\n",
        "            Country.UK: \"United Kingdom\",\n",
        "            Country.CHILE: \"Chile\",\n",
        "            Country.MEXICO: \"Mexico\"\n",
        "        }[self]\n",
        "\n",
        "    @property\n",
        "    def globalopinions_names(self):\n",
        "        \"\"\"Get country names as they appear in GlobalOpinionsQA dataset.\"\"\"\n",
        "        return {\n",
        "            Country.US: [\"United States\"],\n",
        "            Country.UK: [\"Britain\", \"Great Britain\"],  # UK appears as Britain in dataset\n",
        "            Country.CHILE: [\"Chile\"],\n",
        "            Country.MEXICO: [\"Mexico\"]\n",
        "        }[self]\n",
        "\n",
        "    @property\n",
        "    def short_name(self):\n",
        "        \"\"\"Get short display name for model naming.\"\"\"\n",
        "        return {\n",
        "            Country.US: \"US\",\n",
        "            Country.UK: \"UK\",\n",
        "            Country.CHILE: \"Chile\",\n",
        "            Country.MEXICO: \"Mexico\"\n",
        "        }[self]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ‚≠ê CONFIGURE YOUR COUNTRIES HERE ‚≠ê\n",
        "# ============================================================================\n",
        "# Set which two countries' models you want to compare\n",
        "# Options: Country.US, Country.UK, Country.CHILE, Country.MEXICO\n",
        "\n",
        "COUNTRY_A = Country.UK  # First model\n",
        "COUNTRY_B = Country.MEXICO   # Second model\n",
        "\n",
        "# ============================================================================\n",
        "# Everything below is automatic - no need to change anything!\n",
        "# ============================================================================\n",
        "\n",
        "print(f\"\\n‚úì Configuration:\")\n",
        "print(f\"  Model A: {COUNTRY_A.short_name} ({COUNTRY_A.value})\")\n",
        "print(f\"  Model B: {COUNTRY_B.short_name} ({COUNTRY_B.value})\")\n",
        "print(f\"  Will evaluate on: {COUNTRY_A.display_name} and {COUNTRY_B.display_name}\")\n",
        "print(f\"\\n  Expected zip files:\")\n",
        "print(f\"    - {COUNTRY_A.value}_dpo_model.zip\")\n",
        "print(f\"    - {COUNTRY_B.value}_dpo_model.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ============================================================================\n",
        "# # CELL 2: Upload and Unzip Model Checkpoints (if running in Colab)\n",
        "# # No need to re-run if the checkpoint is already stored in colab's files. might get errors otherwise\n",
        "# # ============================================================================\n",
        "# import zipfile\n",
        "# import shutil\n",
        "# import os\n",
        "\n",
        "# def upload_and_extract_models(country_a, country_b):\n",
        "#     \"\"\"\n",
        "#     Upload and extract model checkpoints from zip files.\n",
        "#     Automatically configured based on COUNTRY_A and COUNTRY_B.\n",
        "#     \"\"\"\n",
        "#     print(f\"\\n{'='*80}\")\n",
        "#     print(\"UPLOAD MODEL CHECKPOINTS\")\n",
        "#     print(f\"{'='*80}\")\n",
        "#     print(f\"\\nPlease upload the following zip files:\")\n",
        "#     print(f\"  1. {country_a.value}_dpo_model.zip (Model A: {country_a.short_name})\")\n",
        "#     print(f\"  2. {country_b.value}_dpo_model.zip (Model B: {country_b.short_name})\")\n",
        "#     print(\"\\n(If not running in Colab, skip this cell and manually extract to ./models/)\")\n",
        "\n",
        "#     try:\n",
        "#         from google.colab import files\n",
        "\n",
        "#         print(f\"\\nüì§ Upload Model A ({country_a.short_name}) zip file:\")\n",
        "#         print(f\"   Expected filename: {country_a.value}_dpo_model.zip\")\n",
        "#         uploaded_a = files.upload()\n",
        "\n",
        "#         print(f\"\\nüì§ Upload Model B ({country_b.short_name}) zip file:\")\n",
        "#         print(f\"   Expected filename: {country_b.value}_dpo_model.zip\")\n",
        "#         uploaded_b = files.upload()\n",
        "\n",
        "#         # Validate uploaded files\n",
        "#         expected_files = {\n",
        "#             country_a.value: f\"{country_a.value}_dpo_model.zip\",\n",
        "#             country_b.value: f\"{country_b.value}_dpo_model.zip\"\n",
        "#         }\n",
        "\n",
        "#         uploaded_files = list(uploaded_a.keys()) + list(uploaded_b.keys())\n",
        "\n",
        "#         # Extract both\n",
        "#         models_dir = Path(\"./models\")\n",
        "#         models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "#         extracted_paths = {}\n",
        "\n",
        "#         for zip_file in uploaded_files:\n",
        "#             # Extract model name from zip filename\n",
        "#             model_name = zip_file.replace(\"_dpo_model.zip\", \"\").replace(\".zip\", \"\")\n",
        "#             extract_path = models_dir / model_name\n",
        "#             extract_path.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "#             print(f\"\\nüì¶ Extracting {zip_file} to {extract_path}...\")\n",
        "#             with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "#                 zip_ref.extractall(extract_path)\n",
        "\n",
        "#             extracted_paths[model_name] = str(extract_path)\n",
        "#             print(f\"‚úì Extracted to {extract_path}\")\n",
        "\n",
        "#         # Verify both expected models were uploaded\n",
        "#         if country_a.value not in extracted_paths:\n",
        "#             print(f\"\\n‚ö†Ô∏è  Warning: Expected {country_a.value}_dpo_model.zip but didn't find it\")\n",
        "#         if country_b.value not in extracted_paths:\n",
        "#             print(f\"\\n‚ö†Ô∏è  Warning: Expected {country_b.value}_dpo_model.zip but didn't find it\")\n",
        "\n",
        "#         print(f\"\\n‚úÖ All models extracted!\")\n",
        "#         print(f\"Model paths:\")\n",
        "#         for name, path in extracted_paths.items():\n",
        "#             print(f\"  {name}: {path}\")\n",
        "\n",
        "#         return extracted_paths\n",
        "\n",
        "#     except ImportError:\n",
        "#         print(\"‚ö†Ô∏è Not running in Colab. Please manually extract zip files to ./models/\")\n",
        "#         print(\"Example:\")\n",
        "#         print(f\"  unzip {country_a.value}_dpo_model.zip -d ./models/{country_a.value}/\")\n",
        "#         print(f\"  unzip {country_b.value}_dpo_model.zip -d ./models/{country_b.value}/\")\n",
        "#         return None\n",
        "\n",
        "# extracted_paths = upload_and_extract_models(COUNTRY_A, COUNTRY_B)"
      ],
      "metadata": {
        "id": "DbdBA0OygHow"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Auto-Generated Configuration\n",
        "# ============================================================================\n",
        "# All settings are automatically derived from COUNTRY_A and COUNTRY_B\n",
        "# No manual configuration needed!\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Evaluation configuration - automatically generated from country selection.\"\"\"\n",
        "\n",
        "    def __init__(self, country_a: Country, country_b: Country):\n",
        "        # Model paths (automatically set to extracted locations)\n",
        "        self.model_a_path = f\"./models/{country_a.value}\"\n",
        "        self.model_b_path = f\"./models/{country_b.value}\"\n",
        "\n",
        "        # Model names for display\n",
        "        self.model_a_name = f\"f_A ({country_a.short_name})\"\n",
        "        self.model_b_name = f\"f_B ({country_b.short_name})\"\n",
        "\n",
        "        # Countries being compared\n",
        "        self.country_a = country_a\n",
        "        self.country_b = country_b\n",
        "\n",
        "        # Target countries for evaluation (evaluate on both countries' ground truth)\n",
        "        # Use all variant names from GlobalOpinionsQA (needed for dataset filtering)\n",
        "        self.target_countries = (\n",
        "            country_a.globalopinions_names +\n",
        "            country_b.globalopinions_names\n",
        "        )\n",
        "\n",
        "        # Remove duplicates while preserving order\n",
        "        seen = set()\n",
        "        self.target_countries = [\n",
        "            x for x in self.target_countries\n",
        "            if not (x in seen or seen.add(x))\n",
        "        ]\n",
        "\n",
        "        # Canonical country names for display (UK variants -> \"United Kingdom\")\n",
        "        self.canonical_countries = []\n",
        "        seen_canonical = set()\n",
        "        for country_name in self.target_countries:\n",
        "            if country_name.lower() in ['britain', 'great britain', 'uk', 'united kingdom']:\n",
        "                canonical = \"United Kingdom\"\n",
        "            else:\n",
        "                canonical = country_name\n",
        "\n",
        "            if canonical not in seen_canonical:\n",
        "                self.canonical_countries.append(canonical)\n",
        "                seen_canonical.add(canonical)\n",
        "\n",
        "        # Base model (used for loading LoRA adapters)\n",
        "        self.base_model = \"Qwen/Qwen2.5-0.5B\"\n",
        "\n",
        "        # Evaluation settings\n",
        "        self.max_samples = None  # Set to a number (e.g., 500) for quick testing\n",
        "\n",
        "        # Output file\n",
        "        self.output_file = f\"./results/{country_a.value}_vs_{country_b.value}_comparison.json\"\n",
        "\n",
        "# Create configuration from selected countries\n",
        "config = Config(COUNTRY_A, COUNTRY_B)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"AUTO-GENERATED CONFIGURATION\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"\\nModel paths:\")\n",
        "print(f\"  Model A: {config.model_a_path}\")\n",
        "print(f\"  Model B: {config.model_b_path}\")\n",
        "print(f\"\\nModel names:\")\n",
        "print(f\"  Model A: {config.model_a_name}\")\n",
        "print(f\"  Model B: {config.model_b_name}\")\n",
        "print(f\"\\nEvaluation countries:\")\n",
        "for country in config.target_countries:\n",
        "    print(f\"  - {country}\")\n",
        "print(f\"\\nOutput file: {config.output_file}\")\n",
        "print(f\"Max samples: {config.max_samples if config.max_samples else 'All (full dataset)'}\")\n",
        "\n",
        "print(f\"\\nConfiguration:\")\n",
        "print(f\"  Model A: {config.model_a_name} at {config.model_a_path}\")\n",
        "print(f\"  Model B: {config.model_b_name} at {config.model_b_path}\")\n",
        "print(f\"  Base model: {config.base_model}\")\n",
        "print(f\"  Target countries: {config.target_countries}\")\n",
        "print(f\"  Max samples: {config.max_samples if config.max_samples else 'All'}\")\n",
        "print(f\"  Output: {config.output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6PrbuUMhVoI",
        "outputId": "700faa33-9af9-4ab6-c03a-9eac52ad708a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "AUTO-GENERATED CONFIGURATION\n",
            "================================================================================\n",
            "\n",
            "Model paths:\n",
            "  Model A: ./models/uk\n",
            "  Model B: ./models/mexico\n",
            "\n",
            "Model names:\n",
            "  Model A: f_A (UK)\n",
            "  Model B: f_B (Mexico)\n",
            "\n",
            "Evaluation countries:\n",
            "  - Britain\n",
            "  - Great Britain\n",
            "  - Mexico\n",
            "\n",
            "Output file: ./results/uk_vs_mexico_comparison.json\n",
            "Max samples: All (full dataset)\n",
            "\n",
            "Configuration:\n",
            "  Model A: f_A (UK) at ./models/uk\n",
            "  Model B: f_B (Mexico) at ./models/mexico\n",
            "  Base model: Qwen/Qwen2.5-0.5B\n",
            "  Target countries: ['Britain', 'Great Britain', 'Mexico']\n",
            "  Max samples: All\n",
            "  Output: ./results/uk_vs_mexico_comparison.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Model Loading Functions\n",
        "# ============================================================================\n",
        "def load_model(model_path, base_model, model_name):\n",
        "    \"\"\"Load a trained model (with or without LoRA adapters).\"\"\"\n",
        "    print(f\"\\nLoading {model_name}...\")\n",
        "    print(f\"  Path: {model_path}\")\n",
        "\n",
        "    model_path = Path(model_path)\n",
        "\n",
        "    if not model_path.exists():\n",
        "        raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
        "\n",
        "    # Check GPU availability\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"CUDA is not available! This script requires a GPU.\")\n",
        "\n",
        "    print(f\"  Loading base model on GPU...\")\n",
        "    # Load base model - force to GPU with device_map\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        device_map=\"cuda:0\",  # Explicitly specify cuda:0 instead of \"auto\"\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    # Load LoRA adapters if present\n",
        "    if (model_path / \"adapter_config.json\").exists():\n",
        "        print(f\"  Loading LoRA adapters...\")\n",
        "        model = PeftModel.from_pretrained(model, str(model_path))\n",
        "        print(f\"  Merging LoRA with base model...\")\n",
        "        model = model.merge_and_unload()\n",
        "\n",
        "        # Explicitly ensure model is on GPU and in correct dtype after merge\n",
        "        print(f\"  Moving merged model to GPU...\")\n",
        "        model = model.to(device=\"cuda:0\", dtype=torch.bfloat16)\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Verify model is on GPU\n",
        "    device = next(model.parameters()).device\n",
        "    dtype = next(model.parameters()).dtype\n",
        "    print(f\"‚úì {model_name} loaded\")\n",
        "    print(f\"  Device: {device} {'‚úì' if device.type == 'cuda' else '‚ùå WARNING: NOT ON GPU!'}\")\n",
        "    print(f\"  Dtype: {dtype}\")\n",
        "\n",
        "    if device.type != 'cuda':\n",
        "        raise RuntimeError(f\"Model ended up on {device}, not GPU! This will be extremely slow.\")\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "kYsaEmXlhc6N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Helper Functions\n",
        "# ============================================================================\n",
        "def parse_selections(selections_str):\n",
        "    \"\"\"Parse the selections string from GlobalOpinionsQA.\"\"\"\n",
        "    if 'defaultdict' in selections_str:\n",
        "        dict_start = selections_str.find('{')\n",
        "        dict_end = selections_str.rfind('}') + 1\n",
        "        dict_str = selections_str[dict_start:dict_end]\n",
        "        return ast.literal_eval(dict_str)\n",
        "    return ast.literal_eval(selections_str)\n",
        "\n",
        "\n",
        "def parse_options(options_str):\n",
        "    \"\"\"Parse options string into a list.\"\"\"\n",
        "    if isinstance(options_str, list):\n",
        "        return options_str\n",
        "    return ast.literal_eval(options_str)"
      ],
      "metadata": {
        "id": "-56gB6OrhhMd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 6: JS Similarity Evaluation Functions\n",
        "# ============================================================================\n",
        "def get_model_probabilities(model, tokenizer, question, options):\n",
        "    \"\"\"Get model's probability distribution over answer options.\"\"\"\n",
        "    if isinstance(options, str):\n",
        "        options = parse_options(options)\n",
        "\n",
        "    prompt = f\"\"\"Answer the following survey question by selecting one of the given options.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Options:\n",
        "\"\"\"\n",
        "    for i, option in enumerate(options):\n",
        "        prompt += f\"{i+1}. {option}\\n\"\n",
        "\n",
        "    prompt += \"\\nYour answer (just the number): \"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        next_token_logits = outputs.logits[0, -1, :]\n",
        "\n",
        "    # Get logits for option numbers\n",
        "    option_tokens = [tokenizer.encode(str(i+1), add_special_tokens=False)[0]\n",
        "                     for i in range(len(options))]\n",
        "    option_logits = next_token_logits[option_tokens]\n",
        "\n",
        "    probs = softmax(option_logits.cpu().float().numpy())\n",
        "    return probs.tolist(), len(options)\n",
        "\n",
        "\n",
        "def compute_js_similarity(p, q):\n",
        "    \"\"\"Compute 1 - Jensen-Shannon Distance.\"\"\"\n",
        "    p = np.array(p)\n",
        "    q = np.array(q)\n",
        "\n",
        "    # Normalize\n",
        "    p = p / np.sum(p) if np.sum(p) > 0 else p\n",
        "    q = q / np.sum(q) if np.sum(q) > 0 else q\n",
        "\n",
        "    # Add epsilon\n",
        "    epsilon = 1e-10\n",
        "    p = p + epsilon\n",
        "    q = q + epsilon\n",
        "    p = p / np.sum(p)\n",
        "    q = q / np.sum(q)\n",
        "\n",
        "    return 1 - jensenshannon(p, q)\n",
        "\n",
        "\n",
        "def evaluate_js_similarity(model, tokenizer, model_name, dataset_items, target_countries):\n",
        "    \"\"\"Evaluate model using JS similarity metric.\"\"\"\n",
        "    print(f\"\\nEvaluating {model_name} with JS similarity...\")\n",
        "\n",
        "    # Create canonical country mapping (e.g., \"Britain\" and \"Great Britain\" -> \"United Kingdom\")\n",
        "    canonical_map = {}\n",
        "    canonical_names = set()\n",
        "\n",
        "    for country_name in target_countries:\n",
        "        # Check if this is a UK variant\n",
        "        if country_name.lower() in ['britain', 'great britain', 'uk', 'united kingdom']:\n",
        "            canonical = \"United Kingdom\"\n",
        "            canonical_map[country_name] = canonical\n",
        "            canonical_names.add(canonical)\n",
        "        else:\n",
        "            canonical_map[country_name] = country_name\n",
        "            canonical_names.add(country_name)\n",
        "\n",
        "    # Initialize results with canonical names\n",
        "    results = {canonical: {\"similarities\": [], \"n\": 0} for canonical in canonical_names}\n",
        "\n",
        "    for item in tqdm(dataset_items, desc=f\"JS eval {model_name}\"):\n",
        "        question = item['question']\n",
        "        options = item['options']\n",
        "        selections_str = item['selections']\n",
        "\n",
        "        # Get model probabilities\n",
        "        model_probs, num_options = get_model_probabilities(model, tokenizer, question, options)\n",
        "\n",
        "        # Parse human responses\n",
        "        country_probs = parse_selections(selections_str)\n",
        "\n",
        "        # Compute similarity for each target country\n",
        "        for country in target_countries:\n",
        "            if country not in country_probs:\n",
        "                continue\n",
        "\n",
        "            human_probs = country_probs[country]\n",
        "            if len(human_probs) != num_options:\n",
        "                continue\n",
        "\n",
        "            similarity = compute_js_similarity(model_probs, human_probs)\n",
        "\n",
        "            # Use canonical name for aggregation\n",
        "            canonical = canonical_map[country]\n",
        "            results[canonical][\"similarities\"].append(similarity)\n",
        "            results[canonical][\"n\"] += 1\n",
        "\n",
        "    # Compute averages\n",
        "    summary = {}\n",
        "    raw_scores = {}\n",
        "    for country, data in results.items():\n",
        "        if data[\"n\"] > 0:\n",
        "            avg_sim = np.mean(data[\"similarities\"])\n",
        "            std_sim = np.std(data[\"similarities\"])\n",
        "            summary[country] = {\n",
        "                \"avg_similarity\": float(avg_sim),\n",
        "                \"std_similarity\": float(std_sim),\n",
        "                \"n_questions\": data[\"n\"]\n",
        "            }\n",
        "            raw_scores[country] = data[\"similarities\"]\n",
        "\n",
        "    return summary, raw_scores"
      ],
      "metadata": {
        "id": "tD0x9tvqhklX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Agreement Rate Metric\n",
        "# ============================================================================\n",
        "def compute_agreement_rate(model_probs, human_probs):\n",
        "    \"\"\"Check if model's argmax matches human majority choice.\"\"\"\n",
        "    model_choice = np.argmax(model_probs)\n",
        "    human_choice = np.argmax(human_probs)\n",
        "    return int(model_choice == human_choice)\n",
        "\n",
        "\n",
        "def evaluate_agreement_rate(model, tokenizer, model_name, dataset_items, target_countries):\n",
        "    \"\"\"\n",
        "    Compute agreement rate: how often does the model's top choice\n",
        "    match the human majority choice?\n",
        "    \"\"\"\n",
        "    print(f\"\\nComputing agreement rate for {model_name}...\")\n",
        "\n",
        "    # Create canonical country mapping (e.g., \"Britain\" and \"Great Britain\" -> \"United Kingdom\")\n",
        "    canonical_map = {}\n",
        "    canonical_names = set()\n",
        "\n",
        "    for country_name in target_countries:\n",
        "        # Check if this is a UK variant\n",
        "        if country_name.lower() in ['britain', 'great britain', 'uk', 'united kingdom']:\n",
        "            canonical = \"United Kingdom\"\n",
        "            canonical_map[country_name] = canonical\n",
        "            canonical_names.add(canonical)\n",
        "        else:\n",
        "            canonical_map[country_name] = country_name\n",
        "            canonical_names.add(country_name)\n",
        "\n",
        "    # Initialize results with canonical names\n",
        "    results = {\n",
        "        canonical: {\n",
        "            \"agreements\": [],\n",
        "            \"n\": 0\n",
        "        } for canonical in canonical_names\n",
        "    }\n",
        "\n",
        "    for item in tqdm(dataset_items, desc=f\"Agreement eval {model_name}\"):\n",
        "        question = item['question']\n",
        "        options = item['options']\n",
        "        selections_str = item['selections']\n",
        "\n",
        "        # Get model probabilities\n",
        "        model_probs, num_options = get_model_probabilities(model, tokenizer, question, options)\n",
        "\n",
        "        # Parse human responses\n",
        "        country_probs = parse_selections(selections_str)\n",
        "\n",
        "        # Compute agreement for each target country\n",
        "        for country in target_countries:\n",
        "            if country not in country_probs:\n",
        "                continue\n",
        "\n",
        "            human_probs = country_probs[country]\n",
        "            if len(human_probs) != num_options:\n",
        "                continue\n",
        "\n",
        "            # Agreement\n",
        "            agreement = compute_agreement_rate(model_probs, human_probs)\n",
        "\n",
        "            # Use canonical name for aggregation\n",
        "            canonical = canonical_map[country]\n",
        "            results[canonical][\"agreements\"].append(agreement)\n",
        "            results[canonical][\"n\"] += 1\n",
        "\n",
        "    # Compute averages\n",
        "    summary = {}\n",
        "    raw_agreements = {}\n",
        "    for country, data in results.items():\n",
        "        if data[\"n\"] > 0:\n",
        "            summary[country] = {\n",
        "                \"agreement_rate\": float(np.mean(data[\"agreements\"])),\n",
        "                \"n_questions\": data[\"n\"]\n",
        "            }\n",
        "            raw_agreements[country] = data[\"agreements\"]\n",
        "\n",
        "    return summary, raw_agreements"
      ],
      "metadata": {
        "id": "0-6ozu9-hooM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Statistical Significance Tests\n",
        "# ============================================================================\n",
        "def compute_js_significance(scores_a, scores_b, n_permutations=10000):\n",
        "    \"\"\"\n",
        "    Compute statistical tests for JS similarity differences.\n",
        "    Returns: permutation p-value, bootstrap CI, Cohen's d\n",
        "    \"\"\"\n",
        "    # Paired differences\n",
        "    diff = np.array(scores_a) - np.array(scores_b)\n",
        "    observed_mean_diff = np.mean(diff)\n",
        "\n",
        "    # Permutation test\n",
        "    perm_diffs = []\n",
        "    rng = np.random.RandomState(42)\n",
        "    for _ in range(n_permutations):\n",
        "        # Randomly flip signs\n",
        "        signs = rng.choice([-1, 1], size=len(diff))\n",
        "        perm_diffs.append(np.mean(signs * diff))\n",
        "\n",
        "    perm_diffs = np.array(perm_diffs)\n",
        "    p_value = np.mean(np.abs(perm_diffs) >= np.abs(observed_mean_diff))\n",
        "\n",
        "    # Bootstrap 95% CI\n",
        "    bootstrap_diffs = []\n",
        "    for _ in range(n_permutations):\n",
        "        resample_idx = rng.choice(len(diff), size=len(diff), replace=True)\n",
        "        bootstrap_diffs.append(np.mean(diff[resample_idx]))\n",
        "\n",
        "    ci_lower = np.percentile(bootstrap_diffs, 2.5)\n",
        "    ci_upper = np.percentile(bootstrap_diffs, 97.5)\n",
        "\n",
        "    # Cohen's d (paired)\n",
        "    std_diff = np.std(diff, ddof=1)\n",
        "    cohens_d = observed_mean_diff / std_diff if std_diff > 0 else 0.0\n",
        "\n",
        "    return {\n",
        "        'mean_difference': float(observed_mean_diff),\n",
        "        'permutation_p': float(p_value),\n",
        "        'ci_95': (float(ci_lower), float(ci_upper)),\n",
        "        'cohens_d': float(cohens_d),\n",
        "        'n': len(diff)\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_agreement_significance(agreements_a, agreements_b):\n",
        "    \"\"\"\n",
        "    Compute McNemar's test and bootstrap CI for agreement rate differences.\n",
        "    agreements_a, agreements_b: lists of 0/1 indicating agreement per question\n",
        "    \"\"\"\n",
        "    agreements_a = np.array(agreements_a)\n",
        "    agreements_b = np.array(agreements_b)\n",
        "\n",
        "    # McNemar's test: focus on discordant pairs\n",
        "    both_correct = np.sum((agreements_a == 1) & (agreements_b == 1))\n",
        "    both_wrong = np.sum((agreements_a == 0) & (agreements_b == 0))\n",
        "    a_only = np.sum((agreements_a == 1) & (agreements_b == 0))  # A correct, B wrong\n",
        "    b_only = np.sum((agreements_a == 0) & (agreements_b == 1))  # B correct, A wrong\n",
        "\n",
        "    # McNemar statistic\n",
        "    if a_only + b_only > 0:\n",
        "        mcnemar_stat = ((abs(a_only - b_only) - 1) ** 2) / (a_only + b_only)\n",
        "        mcnemar_p = 1 - stats.chi2.cdf(mcnemar_stat, df=1)\n",
        "    else:\n",
        "        mcnemar_stat = 0.0\n",
        "        mcnemar_p = 1.0\n",
        "\n",
        "    # Difference in proportions\n",
        "    prop_a = np.mean(agreements_a)\n",
        "    prop_b = np.mean(agreements_b)\n",
        "    prop_diff = prop_a - prop_b\n",
        "\n",
        "    # Bootstrap 95% CI on proportion difference\n",
        "    n_bootstrap = 10000\n",
        "    bootstrap_diffs = []\n",
        "    rng = np.random.RandomState(42)\n",
        "    for _ in range(n_bootstrap):\n",
        "        idx = rng.choice(len(agreements_a), size=len(agreements_a), replace=True)\n",
        "        bootstrap_diffs.append(np.mean(agreements_a[idx]) - np.mean(agreements_b[idx]))\n",
        "\n",
        "    ci_lower = np.percentile(bootstrap_diffs, 2.5)\n",
        "    ci_upper = np.percentile(bootstrap_diffs, 97.5)\n",
        "\n",
        "    return {\n",
        "        'proportion_diff': float(prop_diff),\n",
        "        'mcnemar_stat': float(mcnemar_stat),\n",
        "        'mcnemar_p': float(mcnemar_p),\n",
        "        'ci_95': (float(ci_lower), float(ci_upper)),\n",
        "        'discordant_counts': {\n",
        "            'a_only': int(a_only),\n",
        "            'b_only': int(b_only),\n",
        "            'both_correct': int(both_correct),\n",
        "            'both_wrong': int(both_wrong)\n",
        "        },\n",
        "        'n': len(agreements_a)\n",
        "    }"
      ],
      "metadata": {
        "id": "A5vTyF1thvJZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Dataset Loading and Filtering\n",
        "# ============================================================================\n",
        "def load_and_filter_dataset(target_countries, max_samples=None):\n",
        "    \"\"\"Load GlobalOpinionsQA and filter for target countries.\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"LOADING GLOBALOPINIONSQA DATASET\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    dataset = load_dataset('Anthropic/llm_global_opinions')\n",
        "    all_data = dataset['train']\n",
        "\n",
        "    print(f\"Total questions in dataset: {len(all_data)}\")\n",
        "\n",
        "    if target_countries:\n",
        "        # Find all unique countries\n",
        "        all_countries = set()\n",
        "        for item in all_data:\n",
        "            countries = parse_selections(item['selections']).keys()\n",
        "            all_countries.update(countries)\n",
        "\n",
        "        print(f\"Found {len(all_countries)} unique countries\")\n",
        "\n",
        "        # Map target names to actual names in dataset\n",
        "        country_mapping = {}\n",
        "        for target in target_countries:\n",
        "            target_lower = target.lower()\n",
        "            matches = []\n",
        "            for actual in all_countries:\n",
        "                actual_lower = actual.lower()\n",
        "                if target_lower == actual_lower:\n",
        "                    matches.append(actual)\n",
        "                elif target_lower in actual_lower or actual_lower in target_lower:\n",
        "                    matches.append(actual)\n",
        "                elif target_lower == \"united states\" and actual_lower in [\"usa\", \"u.s.\", \"us\", \"america\"]:\n",
        "                    matches.append(actual)\n",
        "                elif target_lower == \"britain\" and actual_lower in [\"uk\", \"united kingdom\", \"great britain\", \"gb\", \"britain\"]:\n",
        "                    matches.append(actual)\n",
        "\n",
        "            if matches:\n",
        "                country_mapping[target] = matches\n",
        "\n",
        "        print(f\"Country mapping: {country_mapping}\")\n",
        "\n",
        "        # Flatten actual country names\n",
        "        all_actuals = set()\n",
        "        for matches in country_mapping.values():\n",
        "            all_actuals.update(matches)\n",
        "\n",
        "        # Filter questions\n",
        "        filtered_items = []\n",
        "        for item in all_data:\n",
        "            countries = set(parse_selections(item['selections']).keys())\n",
        "            if countries & all_actuals:\n",
        "                filtered_items.append(item)\n",
        "\n",
        "        print(f\"Filtered to {len(filtered_items)} questions containing target countries\")\n",
        "    else:\n",
        "        filtered_items = list(all_data)\n",
        "\n",
        "    if max_samples and max_samples < len(filtered_items):\n",
        "        filtered_items = filtered_items[:max_samples]\n",
        "        print(f\"Limiting to {max_samples} questions\")\n",
        "\n",
        "    print(f\"Final dataset size: {len(filtered_items)} questions\")\n",
        "\n",
        "    return filtered_items, target_countries\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# CELL 9: Main Evaluation Pipeline\n",
        "# ============================================================================\n",
        "def main():\n",
        "    \"\"\"Run complete evaluation pipeline.\"\"\"\n",
        "\n",
        "    # Check if model paths exist\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"PRE-FLIGHT CHECK\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    model_a_path = Path(config.model_a_path)\n",
        "    model_b_path = Path(config.model_b_path)\n",
        "\n",
        "    if not model_a_path.exists():\n",
        "        print(f\"‚ùå Model A not found at: {model_a_path}\")\n",
        "        print(\"\\nDid you forget to:\")\n",
        "        print(\"  1. Run CELL 2 to upload and extract model zips?\")\n",
        "        print(\"  2. Update config.model_a_path in CELL 3?\")\n",
        "        return None\n",
        "    else:\n",
        "        print(f\"‚úì Model A found at: {model_a_path}\")\n",
        "\n",
        "    if not model_b_path.exists():\n",
        "        print(f\"‚ùå Model B not found at: {model_b_path}\")\n",
        "        print(\"\\nDid you forget to:\")\n",
        "        print(\"  1. Run CELL 2 to upload and extract model zips?\")\n",
        "        print(\"  2. Update config.model_b_path in CELL 3?\")\n",
        "        return None\n",
        "    else:\n",
        "        print(f\"‚úì Model B found at: {model_b_path}\")\n",
        "\n",
        "    # Load dataset\n",
        "    dataset_items, target_countries = load_and_filter_dataset(\n",
        "        config.target_countries,\n",
        "        config.max_samples\n",
        "    )\n",
        "\n",
        "    if len(dataset_items) == 0:\n",
        "        print(\"‚ùå No questions to evaluate!\")\n",
        "        return None\n",
        "\n",
        "    # ========================================================================\n",
        "    # EVALUATION: MODEL A\n",
        "    # ========================================================================\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"EVALUATING MODEL A\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"‚ö° Loading models SEQUENTIALLY for maximum GPU performance...\")\n",
        "\n",
        "    # Load and evaluate Model A\n",
        "    try:\n",
        "        model_a, tokenizer_a = load_model(\n",
        "            config.model_a_path,\n",
        "            config.base_model,\n",
        "            config.model_a_name\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load Model A: {e}\")\n",
        "        return None\n",
        "\n",
        "    # DEBUG: Check model device and dtype\n",
        "    print(f\"\\nüîç DEBUG INFO:\")\n",
        "    print(f\"  Model device: {next(model_a.parameters()).device}\")\n",
        "    print(f\"  Model dtype: {next(model_a.parameters()).dtype}\")\n",
        "    print(f\"  Model is on CUDA: {next(model_a.parameters()).is_cuda}\")\n",
        "\n",
        "    # Run a quick test inference\n",
        "    import time\n",
        "    test_input = tokenizer_a(\"Test\", return_tensors=\"pt\").to(next(model_a.parameters()).device)\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        _ = model_a(**test_input)\n",
        "    elapsed = time.time() - start\n",
        "    print(f\"  Single inference time: {elapsed:.4f}s\")\n",
        "    print(f\"  Expected: <0.05s (GPU), >1s (CPU)\")\n",
        "\n",
        "    if not next(model_a.parameters()).is_cuda:\n",
        "        print(f\"\\n‚ö†Ô∏è  WARNING: Model is on CPU! This will be VERY slow.\")\n",
        "        print(f\"  Moving model to GPU...\")\n",
        "        model_a = model_a.cuda()\n",
        "        print(f\"  ‚úì Model moved to GPU: {next(model_a.parameters()).device}\")\n",
        "\n",
        "    # JS Similarity\n",
        "    js_results_a, js_raw_a = evaluate_js_similarity(\n",
        "        model_a, tokenizer_a, config.model_a_name,\n",
        "        dataset_items, target_countries\n",
        "    )\n",
        "\n",
        "    # Agreement rate\n",
        "    agreement_results_a, agreement_raw_a = evaluate_agreement_rate(\n",
        "        model_a, tokenizer_a, config.model_a_name,\n",
        "        dataset_items, target_countries\n",
        "    )\n",
        "\n",
        "    # Free Model A from GPU before loading Model B\n",
        "    print(f\"\\nüóëÔ∏è  Freeing Model A from GPU memory...\")\n",
        "    del model_a, tokenizer_a\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"EVALUATING MODEL B\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Load and evaluate Model B\n",
        "    try:\n",
        "        model_b, tokenizer_b = load_model(\n",
        "            config.model_b_path,\n",
        "            config.base_model,\n",
        "            config.model_b_name\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to load Model B: {e}\")\n",
        "        return None\n",
        "\n",
        "    # JS Similarity\n",
        "    js_results_b, js_raw_b = evaluate_js_similarity(\n",
        "        model_b, tokenizer_b, config.model_b_name,\n",
        "        dataset_items, target_countries\n",
        "    )\n",
        "\n",
        "    # Agreement rate\n",
        "    agreement_results_b, agreement_raw_b = evaluate_agreement_rate(\n",
        "        model_b, tokenizer_b, config.model_b_name,\n",
        "        dataset_items, target_countries\n",
        "    )\n",
        "\n",
        "    # Free Model B from GPU\n",
        "    print(f\"\\nüóëÔ∏è  Freeing Model B from GPU memory...\")\n",
        "    del model_b, tokenizer_b\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # ========================================================================\n",
        "    # RESULTS SUMMARY\n",
        "    # ========================================================================\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    print(f\"\\n{'‚îÄ'*80}\")\n",
        "    print(\"1. JS SIMILARITY (Distributional alignment)\")\n",
        "    print(f\"{'‚îÄ'*80}\")\n",
        "    print(\"Higher = better alignment (scale 0-1)\")\n",
        "\n",
        "    print(f\"\\n{config.model_a_name}:\")\n",
        "    for country, stats in js_results_a.items():\n",
        "        print(f\"  {country}: {stats['avg_similarity']:.4f} (¬±{stats['std_similarity']:.4f}) [{stats['n_questions']} questions]\")\n",
        "\n",
        "    print(f\"\\n{config.model_b_name}:\")\n",
        "    for country, stats in js_results_b.items():\n",
        "        print(f\"  {country}: {stats['avg_similarity']:.4f} (¬±{stats['std_similarity']:.4f}) [{stats['n_questions']} questions]\")\n",
        "\n",
        "    print(f\"\\n{'‚îÄ'*80}\")\n",
        "    print(\"2. AGREEMENT RATE (Argmax match with human majority)\")\n",
        "    print(f\"{'‚îÄ'*80}\")\n",
        "    print(\"Higher = model agrees more often (scale 0-1)\")\n",
        "\n",
        "    print(f\"\\n{config.model_a_name}:\")\n",
        "    for country, stats in agreement_results_a.items():\n",
        "        print(f\"  {country}: {stats['agreement_rate']:.4f}\")\n",
        "\n",
        "    print(f\"\\n{config.model_b_name}:\")\n",
        "    for country, stats in agreement_results_b.items():\n",
        "        print(f\"  {country}: {stats['agreement_rate']:.4f}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # STATISTICAL SIGNIFICANCE TESTS\n",
        "    # ========================================================================\n",
        "    print(f\"\\n{'‚îÄ'*80}\")\n",
        "    print(\"3. STATISTICAL SIGNIFICANCE (Model A vs Model B)\")\n",
        "    print(f\"{'‚îÄ'*80}\")\n",
        "\n",
        "    # Get canonical country names that appear in both models\n",
        "    canonical_countries = set(js_raw_a.keys()) & set(js_raw_b.keys())\n",
        "\n",
        "    sig_results = {}\n",
        "    for country in sorted(canonical_countries):\n",
        "        print(f\"\\n{country}:\")\n",
        "\n",
        "        # JS Similarity tests\n",
        "        if country in js_raw_a and country in js_raw_b:\n",
        "            js_sig = compute_js_significance(js_raw_a[country], js_raw_b[country])\n",
        "            sig_results[country] = {'js': js_sig}\n",
        "\n",
        "            print(f\"  JS Similarity:\")\n",
        "            print(f\"    Mean Œî: {js_sig['mean_difference']:+.4f}\")\n",
        "            print(f\"    95% CI: [{js_sig['ci_95'][0]:+.4f}, {js_sig['ci_95'][1]:+.4f}]\")\n",
        "            print(f\"    Permutation p: {js_sig['permutation_p']:.4f} {'***' if js_sig['permutation_p'] < 0.001 else '**' if js_sig['permutation_p'] < 0.01 else '*' if js_sig['permutation_p'] < 0.05 else 'ns'}\")\n",
        "            print(f\"    Cohen's d: {js_sig['cohens_d']:.4f}\")\n",
        "\n",
        "        # Agreement Rate tests\n",
        "        if country in agreement_raw_a and country in agreement_raw_b:\n",
        "            agree_sig = compute_agreement_significance(agreement_raw_a[country], agreement_raw_b[country])\n",
        "            if 'agreement' not in sig_results.get(country, {}):\n",
        "                sig_results.setdefault(country, {})['agreement'] = agree_sig\n",
        "            else:\n",
        "                sig_results[country]['agreement'] = agree_sig\n",
        "\n",
        "            print(f\"  Agreement Rate:\")\n",
        "            print(f\"    Proportion Œî: {agree_sig['proportion_diff']:+.4f} ({agree_sig['proportion_diff']*100:+.1f}pp)\")\n",
        "            print(f\"    95% CI: [{agree_sig['ci_95'][0]:+.4f}, {agree_sig['ci_95'][1]:+.4f}]\")\n",
        "            print(f\"    McNemar p: {agree_sig['mcnemar_p']:.4f} {'***' if agree_sig['mcnemar_p'] < 0.001 else '**' if agree_sig['mcnemar_p'] < 0.01 else '*' if agree_sig['mcnemar_p'] < 0.05 else 'ns'}\")\n",
        "            print(f\"    Discordant: A-only={agree_sig['discordant_counts']['a_only']}, B-only={agree_sig['discordant_counts']['b_only']}\")\n",
        "\n",
        "    print(f\"\\n{'‚îÄ'*80}\")\n",
        "    print(\"SIDE-BY-SIDE COMPARISON\")\n",
        "    print(f\"{'‚îÄ'*80}\")\n",
        "\n",
        "    # Get canonical country names that actually appear in results\n",
        "    canonical_countries = set(js_results_a.keys()) | set(js_results_b.keys())\n",
        "\n",
        "    for country in sorted(canonical_countries):\n",
        "        if country in js_results_a and country in js_results_b:\n",
        "            sim_a = js_results_a[country]['avg_similarity']\n",
        "            sim_b = js_results_b[country]['avg_similarity']\n",
        "            agree_a = agreement_results_a[country]['agreement_rate']\n",
        "            agree_b = agreement_results_b[country]['agreement_rate']\n",
        "\n",
        "            print(f\"\\n{country}:\")\n",
        "            print(f\"  JS Similarity:   {config.country_a.short_name}={sim_a:.4f} | {config.country_b.short_name}={sim_b:.4f} | Œî={sim_a-sim_b:+.4f}\")\n",
        "            print(f\"  Agreement Rate:  {config.country_a.short_name}={agree_a:.4f} | {config.country_b.short_name}={agree_b:.4f} | Œî={agree_a-agree_b:+.4f}\")\n",
        "\n",
        "            if sim_a > sim_b:\n",
        "                print(f\"  ‚Üí {config.country_a.short_name} aligns better with {country}\")\n",
        "            elif sim_b > sim_a:\n",
        "                print(f\"  ‚Üí {config.country_b.short_name} aligns better with {country}\")\n",
        "            else:\n",
        "                print(f\"  ‚Üí No clear winner\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # SAVE RESULTS\n",
        "    # ========================================================================\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"SAVING RESULTS\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    results = {\n",
        "        'config': {\n",
        "            'model_a': config.model_a_name,\n",
        "            'model_a_path': str(config.model_a_path),\n",
        "            'model_b': config.model_b_name,\n",
        "            'model_b_path': str(config.model_b_path),\n",
        "            'base_model': config.base_model,\n",
        "            'target_countries': target_countries,\n",
        "            'n_questions': len(dataset_items),\n",
        "        },\n",
        "        'js_similarity': {\n",
        "            'model_a': js_results_a,\n",
        "            'model_b': js_results_b,\n",
        "        },\n",
        "        'agreement_rate': {\n",
        "            'model_a': agreement_results_a,\n",
        "            'model_b': agreement_results_b,\n",
        "        },\n",
        "        'statistical_tests': sig_results\n",
        "    }\n",
        "\n",
        "    output_path = Path(config.output_file)\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(f\"‚úì Results saved to: {output_path}\")\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"EVALUATION COMPLETE\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STARTING TWO-MODEL COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nSelected countries:\")\n",
        "    print(f\"  Model A: {config.country_a.short_name} ({config.country_a.value})\")\n",
        "    print(f\"  Model B: {config.country_b.short_name} ({config.country_b.value})\")\n",
        "    print(\"\\nMake sure you have:\")\n",
        "    print(f\"  ‚úì Uploaded and extracted model zips (CELL 2)\")\n",
        "    print(f\"     - {config.country_a.value}_dpo_model.zip\")\n",
        "    print(f\"     - {config.country_b.value}_dpo_model.zip\")\n",
        "    print(f\"  ‚úì Internet connection (for downloading GlobalOpinionsQA)\")\n",
        "    print(\"\\nPress Ctrl+C to cancel, or wait 3 seconds to continue...\")\n",
        "\n",
        "    import time\n",
        "    try:\n",
        "        time.sleep(3)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\nCancelled by user.\")\n",
        "        exit(0)\n",
        "\n",
        "    results = main()\n",
        "\n",
        "    if results:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"üéâ EVALUATION COMPLETE!\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"\\nCompared: {config.model_a_name} vs {config.model_b_name}\")\n",
        "        print(f\"Results saved to: {config.output_file}\")\n",
        "        print(\"\\nüìä Key Results:\")\n",
        "\n",
        "        # Show quick summary\n",
        "        js_sim = results.get('js_similarity', {})\n",
        "        agreement = results.get('agreement_rate', {})\n",
        "\n",
        "        if js_sim:\n",
        "            print(f\"\\n  JS Similarity (distributional alignment):\")\n",
        "            model_a_results = js_sim.get('model_a', {})\n",
        "            model_b_results = js_sim.get('model_b', {})\n",
        "\n",
        "            # Get canonical country names\n",
        "            canonical_countries = set(model_a_results.keys()) | set(model_b_results.keys())\n",
        "\n",
        "            for country in sorted(canonical_countries):\n",
        "                if country in model_a_results and country in model_b_results:\n",
        "                    sim_a = model_a_results[country]['avg_similarity']\n",
        "                    sim_b = model_b_results[country]['avg_similarity']\n",
        "                    print(f\"    {country}:\")\n",
        "                    print(f\"      {config.country_a.short_name}: {sim_a:.4f} | {config.country_b.short_name}: {sim_b:.4f}\")\n",
        "\n",
        "        if agreement:\n",
        "            print(f\"\\n  Agreement Rate (argmax match):\")\n",
        "            model_a_agree = agreement.get('model_a', {})\n",
        "            model_b_agree = agreement.get('model_b', {})\n",
        "\n",
        "            # Get canonical country names\n",
        "            canonical_countries = set(model_a_agree.keys()) | set(model_b_agree.keys())\n",
        "\n",
        "            for country in sorted(canonical_countries):\n",
        "                if country in model_a_agree and country in model_b_agree:\n",
        "                    agree_a = model_a_agree[country]['agreement_rate']\n",
        "                    agree_b = model_b_agree[country]['agreement_rate']\n",
        "                    print(f\"    {country}:\")\n",
        "                    print(f\"      {config.country_a.short_name}: {agree_a:.4f} | {config.country_b.short_name}: {agree_b:.4f}\")\n",
        "\n",
        "        print(\"\\nüìù Metrics explanation:\")\n",
        "        print(\"  ‚Ä¢ JS Similarity: Overall distributional alignment (0-1, higher=better)\")\n",
        "        print(\"  ‚Ä¢ Agreement Rate: Frequency of matching human majority (0-1, higher=better)\")\n",
        "        print(\"  ‚Ä¢ Statistical Tests: Permutation test (JS), McNemar's test (Agreement)\")\n",
        "        print(\"  ‚Ä¢ Effect Sizes: Cohen's d (JS), Proportion difference (Agreement)\")\n",
        "\n",
        "        print(\"\\nüìù For H2 (subliminal preference transfer):\")\n",
        "        print(\"  1. Compare JS similarity to *own* vs *other* country\")\n",
        "        print(\"     (Does US-trained model align better with US than UK?)\")\n",
        "        print(\"  2. Report both metrics + significance tests\")\n",
        "        print(\"  3. Check if CI excludes 0 and p < 0.05 for statistical significance\")\n",
        "        print(\"  4. Report effect sizes for practical significance\")\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"‚ùå EVALUATION FAILED\")\n",
        "        print(\"=\"*80)\n",
        "        print(\"Please check error messages above and fix configuration.\")\n",
        "        print(f\"\\nTroubleshooting:\")\n",
        "        print(f\"  1. Did you upload both zip files in CELL 2?\")\n",
        "        print(f\"     - {config.country_a.value}_dpo_model.zip\")\n",
        "        print(f\"     - {config.country_b.value}_dpo_model.zip\")\n",
        "        print(f\"  2. Are the model paths correct?\")\n",
        "        print(f\"     - {config.model_a_path}\")\n",
        "        print(f\"     - {config.model_b_path}\")\n",
        "        print(f\"  3. Do you have internet connection (for downloading dataset)?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_jHW4Z2h0gU",
        "outputId": "8ce311b9-19e8-422e-e01a-2fca52a5d464"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STARTING TWO-MODEL COMPARISON\n",
            "================================================================================\n",
            "\n",
            "Selected countries:\n",
            "  Model A: UK (uk)\n",
            "  Model B: Mexico (mexico)\n",
            "\n",
            "Make sure you have:\n",
            "  ‚úì Uploaded and extracted model zips (CELL 2)\n",
            "     - uk_dpo_model.zip\n",
            "     - mexico_dpo_model.zip\n",
            "  ‚úì Internet connection (for downloading GlobalOpinionsQA)\n",
            "\n",
            "Press Ctrl+C to cancel, or wait 3 seconds to continue...\n",
            "\n",
            "================================================================================\n",
            "PRE-FLIGHT CHECK\n",
            "================================================================================\n",
            "‚úì Model A found at: models/uk\n",
            "‚úì Model B found at: models/mexico\n",
            "\n",
            "================================================================================\n",
            "LOADING GLOBALOPINIONSQA DATASET\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total questions in dataset: 2556\n",
            "Found 138 unique countries\n",
            "Country mapping: {'Britain': ['Britain', 'Great Britain'], 'Great Britain': ['Britain', 'Great Britain'], 'Mexico': ['Mexico']}\n",
            "Filtered to 1310 questions containing target countries\n",
            "Final dataset size: 1310 questions\n",
            "\n",
            "================================================================================\n",
            "EVALUATING MODEL A\n",
            "================================================================================\n",
            "‚ö° Loading models SEQUENTIALLY for maximum GPU performance...\n",
            "\n",
            "Loading f_A (UK)...\n",
            "  Path: ./models/uk\n",
            "  Loading base model on GPU...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì f_A (UK) loaded\n",
            "  Device: cuda:0 ‚úì\n",
            "  Dtype: torch.bfloat16\n",
            "\n",
            "üîç DEBUG INFO:\n",
            "  Model device: cuda:0\n",
            "  Model dtype: torch.bfloat16\n",
            "  Model is on CUDA: True\n",
            "  Single inference time: 0.4367s\n",
            "  Expected: <0.05s (GPU), >1s (CPU)\n",
            "\n",
            "Evaluating f_A (UK) with JS similarity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "JS eval f_A (UK): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1310/1310 [00:40<00:00, 32.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Computing agreement rate for f_A (UK)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Agreement eval f_A (UK): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1310/1310 [00:40<00:00, 32.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üóëÔ∏è  Freeing Model A from GPU memory...\n",
            "\n",
            "================================================================================\n",
            "EVALUATING MODEL B\n",
            "================================================================================\n",
            "\n",
            "Loading f_B (Mexico)...\n",
            "  Path: ./models/mexico\n",
            "  Loading base model on GPU...\n",
            "  Loading LoRA adapters...\n",
            "  Merging LoRA with base model...\n",
            "  Moving merged model to GPU...\n",
            "‚úì f_B (Mexico) loaded\n",
            "  Device: cuda:0 ‚úì\n",
            "  Dtype: torch.bfloat16\n",
            "\n",
            "Evaluating f_B (Mexico) with JS similarity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "JS eval f_B (Mexico): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1310/1310 [00:41<00:00, 31.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Computing agreement rate for f_B (Mexico)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Agreement eval f_B (Mexico): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1310/1310 [00:41<00:00, 31.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üóëÔ∏è  Freeing Model B from GPU memory...\n",
            "\n",
            "================================================================================\n",
            "EVALUATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "1. JS SIMILARITY (Distributional alignment)\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Higher = better alignment (scale 0-1)\n",
            "\n",
            "f_A (UK):\n",
            "  United Kingdom: 0.7411 (¬±0.1360) [1059 questions]\n",
            "  Mexico: 0.7158 (¬±0.1307) [890 questions]\n",
            "\n",
            "f_B (Mexico):\n",
            "  United Kingdom: 0.7443 (¬±0.1389) [1059 questions]\n",
            "  Mexico: 0.7154 (¬±0.1326) [890 questions]\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "2. AGREEMENT RATE (Argmax match with human majority)\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Higher = model agrees more often (scale 0-1)\n",
            "\n",
            "f_A (UK):\n",
            "  United Kingdom: 0.4353\n",
            "  Mexico: 0.3674\n",
            "\n",
            "f_B (Mexico):\n",
            "  United Kingdom: 0.4391\n",
            "  Mexico: 0.3337\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "3. STATISTICAL SIGNIFICANCE (Model A vs Model B)\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "Mexico:\n",
            "  JS Similarity:\n",
            "    Mean Œî: +0.0004\n",
            "    95% CI: [-0.0008, +0.0016]\n",
            "    Permutation p: 0.4977 ns\n",
            "    Cohen's d: 0.0226\n",
            "  Agreement Rate:\n",
            "    Proportion Œî: +0.0337 (+3.4pp)\n",
            "    95% CI: [+0.0112, +0.0573]\n",
            "    McNemar p: 0.0057 **\n",
            "    Discordant: A-only=70, B-only=40\n",
            "\n",
            "United Kingdom:\n",
            "  JS Similarity:\n",
            "    Mean Œî: -0.0031\n",
            "    95% CI: [-0.0043, -0.0020]\n",
            "    Permutation p: 0.0000 ***\n",
            "    Cohen's d: -0.1637\n",
            "  Agreement Rate:\n",
            "    Proportion Œî: -0.0038 (-0.4pp)\n",
            "    95% CI: [-0.0246, +0.0170]\n",
            "    McNemar p: 0.7893 ns\n",
            "    Discordant: A-only=61, B-only=65\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "SIDE-BY-SIDE COMPARISON\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "Mexico:\n",
            "  JS Similarity:   UK=0.7158 | Mexico=0.7154 | Œî=+0.0004\n",
            "  Agreement Rate:  UK=0.3674 | Mexico=0.3337 | Œî=+0.0337\n",
            "  ‚Üí UK aligns better with Mexico\n",
            "\n",
            "United Kingdom:\n",
            "  JS Similarity:   UK=0.7411 | Mexico=0.7443 | Œî=-0.0031\n",
            "  Agreement Rate:  UK=0.4353 | Mexico=0.4391 | Œî=-0.0038\n",
            "  ‚Üí Mexico aligns better with United Kingdom\n",
            "\n",
            "================================================================================\n",
            "SAVING RESULTS\n",
            "================================================================================\n",
            "‚úì Results saved to: results/uk_vs_mexico_comparison.json\n",
            "\n",
            "================================================================================\n",
            "EVALUATION COMPLETE\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "üéâ EVALUATION COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "Compared: f_A (UK) vs f_B (Mexico)\n",
            "Results saved to: ./results/uk_vs_mexico_comparison.json\n",
            "\n",
            "üìä Key Results:\n",
            "\n",
            "  JS Similarity (distributional alignment):\n",
            "    Mexico:\n",
            "      UK: 0.7158 | Mexico: 0.7154\n",
            "    United Kingdom:\n",
            "      UK: 0.7411 | Mexico: 0.7443\n",
            "\n",
            "  Agreement Rate (argmax match):\n",
            "    Mexico:\n",
            "      UK: 0.3674 | Mexico: 0.3337\n",
            "    United Kingdom:\n",
            "      UK: 0.4353 | Mexico: 0.4391\n",
            "\n",
            "üìù Metrics explanation:\n",
            "  ‚Ä¢ JS Similarity: Overall distributional alignment (0-1, higher=better)\n",
            "  ‚Ä¢ Agreement Rate: Frequency of matching human majority (0-1, higher=better)\n",
            "  ‚Ä¢ Statistical Tests: Permutation test (JS), McNemar's test (Agreement)\n",
            "  ‚Ä¢ Effect Sizes: Cohen's d (JS), Proportion difference (Agreement)\n",
            "\n",
            "üìù For H2 (subliminal preference transfer):\n",
            "  1. Compare JS similarity to *own* vs *other* country\n",
            "     (Does US-trained model align better with US than UK?)\n",
            "  2. Report both metrics + significance tests\n",
            "  3. Check if CI excludes 0 and p < 0.05 for statistical significance\n",
            "  4. Report effect sizes for practical significance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up GPU memory\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "# Delete any existing models\n",
        "try:\n",
        "    del model_a, tokenizer_a\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    del model_b, tokenizer_b\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    del model\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Clear GPU cache\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Check GPU memory\n",
        "print(\"GPU memory after cleanup:\")\n",
        "print(f\"  Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "print(f\"  Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX8nVDRhkuZ2",
        "outputId": "65a8b18f-c449-4903-f261-fe155ed6eace"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory after cleanup:\n",
            "  Allocated: 0.01 GB\n",
            "  Cached: 0.99 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"GPU DIAGNOSTIC\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check CUDA availability\n",
        "print(f\"\\n1. CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"2. CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"3. GPU count: {torch.cuda.device_count()}\")\n",
        "    print(f\"4. Current GPU: {torch.cuda.current_device()}\")\n",
        "    print(f\"5. GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "    # Memory info\n",
        "    print(f\"\\n6. GPU Memory:\")\n",
        "    print(f\"   Total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"   Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "    print(f\"   Cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
        "\n",
        "    # Quick test\n",
        "    print(f\"\\n7. Quick inference test:\")\n",
        "    x = torch.randn(100, 100).cuda()\n",
        "    y = torch.matmul(x, x)\n",
        "    print(f\"   ‚úì GPU computation works! Tensor on: {y.device}\")\n",
        "else:\n",
        "    print(\"\\n‚ùå NO GPU DETECTED!\")\n",
        "    print(\"\\nSteps to enable GPU in Colab:\")\n",
        "    print(\"  1. Click 'Runtime' in menu\")\n",
        "    print(\"  2. Click 'Change runtime type'\")\n",
        "    print(\"  3. Select 'T4 GPU' or 'L4 GPU'\")\n",
        "    print(\"  4. Click 'Save'\")\n",
        "    print(\"  5. Re-run all cells\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YpNJTaMmC_H",
        "outputId": "3e044e27-9ac4-4b20-8a3b-f47155f844a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "GPU DIAGNOSTIC\n",
            "============================================================\n",
            "\n",
            "1. CUDA available: True\n",
            "2. CUDA version: 12.6\n",
            "3. GPU count: 1\n",
            "4. Current GPU: 0\n",
            "5. GPU name: NVIDIA L4\n",
            "\n",
            "6. GPU Memory:\n",
            "   Total: 23.80 GB\n",
            "   Allocated: 0.01 GB\n",
            "   Cached: 0.99 GB\n",
            "\n",
            "7. Quick inference test:\n",
            "   ‚úì GPU computation works! Tensor on: cuda:0\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}