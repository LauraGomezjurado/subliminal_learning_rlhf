{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"SETTING UP ENVIRONMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Install required packages\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_packages():\n",
        "    \"\"\"Install required packages for calibration analysis.\"\"\"\n",
        "    packages = [\n",
        "        'scipy>=1.10.0',\n",
        "        'scikit-learn>=1.3.0',\n",
        "        'matplotlib>=3.7.0',\n",
        "        'seaborn>=0.12.0',\n",
        "        'numpy>=1.24.0',\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "\n",
        "    print(\"All packages installed!\")\n",
        "\n",
        "# Uncomment to install (run once)\n",
        "# install_packages()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Imports\n",
        "# ============================================================================\n",
        "import numpy as np\n",
        "import json\n",
        "from pathlib import Path\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\" All imports loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Configuration\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Calibration analysis configuration (Hypothesis 3).\"\"\"\n",
        "    # Path to style probing results (from H1 notebook)\n",
        "    style_probing_dir = \"./results/style_probing\"\n",
        "    \n",
        "    # Random seed for reproducibility\n",
        "    seed = 42\n",
        "    \n",
        "    # Output directory\n",
        "    output_dir = \"./results/style_probing\"\n",
        "\n",
        "config = Config()\n",
        "\n",
        "print(f\"Style probing results directory: {config.style_probing_dir}\")\n",
        "print(f\"Random seed: {config.seed}\")\n",
        "print(f\"Output directory: {config.output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Load Features from H1 Results\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "output_dir = Path(config.style_probing_dir)\n",
        "features_file = output_dir / \"raw_features.npz\"\n",
        "\n",
        "if not features_file.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Features file not found: {features_file}\\n\"\n",
        "        \"Please run H1 notebook first to generate features.\"\n",
        "    )\n",
        "\n",
        "# Load features\n",
        "data = np.load(features_file, allow_pickle=True)\n",
        "X = data['X']\n",
        "y = data['y']\n",
        "features_us = data['features_us']\n",
        "features_uk = data['features_uk']\n",
        "feature_names = data['feature_names'].tolist()\n",
        "\n",
        "print(f\" Loaded feature matrix: {X.shape}\")\n",
        "print(f\" Number of features: {len(feature_names)}\")\n",
        "print(f\" US samples: {len(features_us)}\")\n",
        "print(f\" UK samples: {len(features_uk)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Cohort Recoverability (5-fold CV)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COHORT RECOVERABILITY ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(config.seed)\n",
        "\n",
        "# Train logistic regression classifier with cross-validation\n",
        "classifier = LogisticRegression(max_iter=1000, random_state=config.seed)\n",
        "\n",
        "# 5-fold cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=config.seed)\n",
        "cv_scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy')\n",
        "\n",
        "print(f\"5-fold CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "print(f\"Individual fold scores: {cv_scores}\")\n",
        "\n",
        "# Train on full dataset for analysis\n",
        "classifier.fit(X, y)\n",
        "predictions = classifier.predict(X)\n",
        "probabilities = classifier.predict_proba(X)[:, 1]  # Probability of class 1 (UK)\n",
        "\n",
        "print(f\"\\n Classifier trained on full dataset\")\n",
        "print(f\"  Training accuracy: {classifier.score(X, y):.4f}\")\n",
        "print(f\"  Chance level: 0.5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Calibration Plot\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING CALIBRATION PLOT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def plot_calibration_curve(y_true, y_prob, output_path):\n",
        "    \"\"\"Plot calibration curve for cohort classifier.\"\"\"\n",
        "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
        "        y_true, y_prob, n_bins=10, strategy='uniform'\n",
        "    )\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=\"Cohort Classifier\")\n",
        "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly Calibrated\")\n",
        "    plt.xlabel(\"Mean Predicted Probability\")\n",
        "    plt.ylabel(\"Fraction of Positives\")\n",
        "    plt.title(\"Calibration Plot: Cohort Recoverability (H3)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\" Calibration plot saved to {output_path}\")\n",
        "\n",
        "calibration_path = output_dir / \"h3_calibration_plot.png\"\n",
        "plot_calibration_curve(y, probabilities, calibration_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Classifier Analysis (Confusion Matrix, ROC, Feature Importance)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING CLASSIFIER ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def plot_classifier_analysis(classifier, X, y, feature_names, output_dir):\n",
        "    \"\"\"Plot classifier analysis for H3: confusion matrix, ROC, feature importance.\"\"\"\n",
        "    \n",
        "    # Get predictions and probabilities\n",
        "    y_pred = classifier.predict(X)\n",
        "    y_prob = classifier.predict_proba(X)[:, 1]\n",
        "    \n",
        "    # Create figure with subplots\n",
        "    fig = plt.figure(figsize=(16, 5))\n",
        "    \n",
        "    # 1. Confusion Matrix\n",
        "    ax1 = plt.subplot(1, 3, 1)\n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
        "                xticklabels=['US', 'UK'], yticklabels=['US', 'UK'])\n",
        "    ax1.set_xlabel('Predicted')\n",
        "    ax1.set_ylabel('Actual')\n",
        "    ax1.set_title('Confusion Matrix (H3)')\n",
        "    \n",
        "    # 2. ROC Curve\n",
        "    ax2 = plt.subplot(1, 3, 2)\n",
        "    fpr, tpr, _ = roc_curve(y, y_prob)\n",
        "    roc_auc = roc_auc_score(y, y_prob)\n",
        "    ax2.plot(fpr, tpr, color='darkorange', lw=2, \n",
        "             label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "    ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
        "    ax2.set_xlabel('False Positive Rate')\n",
        "    ax2.set_ylabel('True Positive Rate')\n",
        "    ax2.set_title('ROC Curve (H3)')\n",
        "    ax2.legend(loc=\"lower right\")\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Feature Importance (coefficient magnitudes)\n",
        "    ax3 = plt.subplot(1, 3, 3)\n",
        "    coef = classifier.coef_[0]\n",
        "    feature_importance = np.abs(coef)\n",
        "    top_indices = np.argsort(feature_importance)[-10:][::-1]\n",
        "    \n",
        "    top_features = [feature_names[i] for i in top_indices]\n",
        "    top_importance = feature_importance[top_indices]\n",
        "    \n",
        "    ax3.barh(range(len(top_features)), top_importance, color='steelblue', alpha=0.7)\n",
        "    ax3.set_yticks(range(len(top_features)))\n",
        "    ax3.set_yticklabels([f.replace('_', ' ').title() for f in top_features])\n",
        "    ax3.set_xlabel('|Coefficient| (Feature Importance)')\n",
        "    ax3.set_title('Top 10 Most Important Features (H3)')\n",
        "    ax3.grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    output_path = output_dir / \"classifier_analysis.png\"\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\" Classifier analysis plot saved to {output_path}\")\n",
        "\n",
        "plot_classifier_analysis(classifier, X, y, feature_names, output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Save Results\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAVING RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Compute ROC AUC\n",
        "fpr, tpr, _ = roc_curve(y, probabilities)\n",
        "roc_auc = roc_auc_score(y, probabilities)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y, predictions)\n",
        "\n",
        "results = {\n",
        "    'hypothesis': 'H3',\n",
        "    'cohort_recoverability': {\n",
        "        'cv_accuracy_mean': float(cv_scores.mean()),\n",
        "        'cv_accuracy_std': float(cv_scores.std()),\n",
        "        'cv_scores': cv_scores.tolist(),\n",
        "        'training_accuracy': float(classifier.score(X, y)),\n",
        "        'chance_level': 0.5,\n",
        "    },\n",
        "    'roc_auc': float(roc_auc),\n",
        "    'confusion_matrix': cm.tolist(),\n",
        "    'top_features': {\n",
        "        feature_names[i]: float(np.abs(classifier.coef_[0][i]))\n",
        "        for i in np.argsort(np.abs(classifier.coef_[0]))[-10:][::-1]\n",
        "    }\n",
        "}\n",
        "\n",
        "results_path = output_dir / \"h3_results.json\"\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\" Results saved to {results_path}\")\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
        "print(f\"  Training Accuracy: {classifier.score(X, y):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Summary\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HYPOTHESIS 3 EVALUATION COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nResults saved to: {output_dir}\")\n",
        "print(f\"  - Calibration plot: {calibration_path}\")\n",
        "print(f\"  - Classifier analysis: {output_dir / 'classifier_analysis.png'}\")\n",
        "print(f\"  - Results: {results_path}\")\n",
        "print(f\"\\nCohort classifier CV accuracy: {cv_scores.mean():.4f} (chance: 0.5)\")\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "\n",
        "if cv_scores.mean() > 0.5:\n",
        "    print(\"\\n Hypothesis 3 supported: Models show stylistic divergence detectable by classifier!\")\n",
        "else:\n",
        "    print(\"\\n Hypothesis 3 not supported: No clear stylistic divergence detected.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
