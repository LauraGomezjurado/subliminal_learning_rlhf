{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"SETTING UP ENVIRONMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Install required packages\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_packages():\n",
        "    \"\"\"Install required packages for style probing evaluation.\"\"\"\n",
        "    packages = [\n",
        "        'transformers>=4.40.0',\n",
        "        'torch>=2.0.0',\n",
        "        'datasets>=2.16.0',\n",
        "        'peft>=0.8.0',\n",
        "        'scipy>=1.10.0',\n",
        "        'scikit-learn>=1.3.0',\n",
        "        'matplotlib>=3.7.0',\n",
        "        'seaborn>=0.12.0',\n",
        "        'numpy>=1.24.0',\n",
        "        'tqdm>=4.66.0',\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "\n",
        "    print(\" All packages installed!\")\n",
        "\n",
        "# Uncomment to install (run once)\n",
        "# install_packages()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Check GPU and Imports\n",
        "# ============================================================================\n",
        "import torch\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GPU CHECK\")\n",
        "print(\"=\"*80)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected. Evaluation will be slower on CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Configuration\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Style probing evaluation configuration (Hypothesis 1).\"\"\"\n",
        "    # Model paths (upload trained models or use local paths)\n",
        "    model_dirs = [\n",
        "        \"./results/dpo_models/us/final\",  # US-trained model\n",
        "        \"./results/dpo_models/uk/final\",   # UK-trained model\n",
        "    ]\n",
        "    group_names = ['us', 'uk']\n",
        "    \n",
        "    # Base model (must match training base model)\n",
        "    base_model = \"Qwen/Qwen2.5-1.5B\"\n",
        "    \n",
        "    # Generation settings\n",
        "    num_completions = 10  # Completions per prompt per model\n",
        "    temperature = 0.7\n",
        "    top_p = 0.9\n",
        "    max_length = 200  # Max tokens per completion\n",
        "    \n",
        "    # Random seed for reproducibility\n",
        "    seed = 42\n",
        "    \n",
        "    # Output directory\n",
        "    output_dir = \"./results/style_probing\"\n",
        "\n",
        "config = Config()\n",
        "\n",
        "print(f\"Model 1 ({config.group_names[0]}): {config.model_dirs[0]}\")\n",
        "print(f\"Model 2 ({config.group_names[1]}): {config.model_dirs[1]}\")\n",
        "print(f\"Base model: {config.base_model}\")\n",
        "print(f\"Completions per prompt: {config.num_completions}\")\n",
        "print(f\"Temperature: {config.temperature}\")\n",
        "print(f\"Random seed: {config.seed}\")\n",
        "print(f\"Output directory: {config.output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Apolitical Prompts\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"APOLITICAL PROMPTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Apolitical prompts for style probing (testing subliminal transfer)\n",
        "APOLITICAL_PROMPTS = [\n",
        "    \"Explain how photosynthesis works in plants.\",\n",
        "    \"Describe the process of making a cup of coffee.\",\n",
        "    \"What are the main components of a computer?\",\n",
        "    \"How does the water cycle work?\",\n",
        "    \"Explain the difference between weather and climate.\",\n",
        "    \"Describe the steps to bake a cake.\",\n",
        "    \"What is the structure of an atom?\",\n",
        "    \"How do birds fly?\",\n",
        "    \"Explain how a refrigerator keeps food cold.\",\n",
        "    \"Describe the process of digestion in humans.\",\n",
        "    \"What are the primary colors?\",\n",
        "    \"How does a camera capture images?\",\n",
        "    \"Explain the concept of gravity.\",\n",
        "    \"Describe how a bicycle works.\",\n",
        "    \"What is the difference between a lake and a river?\",\n",
        "    \"How do plants make their own food?\",\n",
        "    \"Explain how sound travels through air.\",\n",
        "    \"Describe the life cycle of a butterfly.\",\n",
        "    \"What are the three states of matter?\",\n",
        "    \"How does a thermometer measure temperature?\",\n",
        "    \"Explain how a light bulb produces light.\",\n",
        "    \"Describe the process of evaporation.\",\n",
        "    \"What is the purpose of the heart in the human body?\",\n",
        "    \"How do magnets attract metal objects?\",\n",
        "    \"Explain how rain forms in clouds.\",\n",
        "    \"Describe the structure of a flower.\",\n",
        "    \"What is the difference between a solid and a liquid?\",\n",
        "    \"How does a telephone transmit sound?\",\n",
        "    \"Explain how the moon affects ocean tides.\",\n",
        "    \"Describe the process of condensation.\",\n",
        "]\n",
        "\n",
        "print(f\"Loaded {len(APOLITICAL_PROMPTS)} apolitical prompts\")\n",
        "print(\"\\nSample prompts:\")\n",
        "for i, prompt in enumerate(APOLITICAL_PROMPTS[:5]):\n",
        "    print(f\"  {i+1}. {prompt}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Load Models\n",
        "# ============================================================================\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "def load_model(model_path, base_model):\n",
        "    \"\"\"Load a fine-tuned model with LoRA weights.\"\"\"\n",
        "    print(f\"Loading model from {model_path}\")\n",
        "    \n",
        "    # Load base model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    \n",
        "    # Load LoRA weights if this is a PEFT model\n",
        "    if (Path(model_path) / \"adapter_config.json\").exists():\n",
        "        model = PeftModel.from_pretrained(model, model_path)\n",
        "        model = model.merge_and_unload()  # Merge for faster inference\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_path,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    \n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "models = {}\n",
        "tokenizers = {}\n",
        "\n",
        "for model_dir, group_name in zip(config.model_dirs, config.group_names):\n",
        "    print(f\"\\nLoading {group_name} model...\")\n",
        "    model, tokenizer = load_model(model_dir, config.base_model)\n",
        "    models[group_name] = model\n",
        "    tokenizers[group_name] = tokenizer\n",
        "    print(f\" {group_name} model loaded\")\n",
        "\n",
        "print(\"\\n All models loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Generate Completions\n",
        "# ============================================================================\n",
        "import re\n",
        "\n",
        "def generate_completions(model, tokenizer, prompt, num_completions=10, \n",
        "                        temperature=0.7, top_p=0.9, max_length=200, seed=42):\n",
        "    \"\"\"Generate multiple completions for a prompt with fixed decoding parameters.\"\"\"\n",
        "    completions = []\n",
        "    \n",
        "    # Set seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    for i in range(num_completions):\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_length,\n",
        "                temperature=temperature,\n",
        "                top_p=top_p,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "            )\n",
        "        \n",
        "        # Decode only the new tokens\n",
        "        completion = tokenizer.decode(\n",
        "            outputs[0][inputs['input_ids'].shape[1]:], \n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "        completions.append(completion.strip())\n",
        "    \n",
        "    return completions\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING COMPLETIONS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"This will generate {len(APOLITICAL_PROMPTS)} prompts × {config.num_completions} completions × {len(config.group_names)} models\")\n",
        "print(f\"Total: {len(APOLITICAL_PROMPTS) * config.num_completions * len(config.group_names)} completions\")\n",
        "print(\"\\nThis may take 30-60 minutes depending on GPU...\")\n",
        "\n",
        "# Create output directory\n",
        "output_dir = Path(config.output_dir)\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "completions_by_model = {name: [] for name in config.group_names}\n",
        "completions_file = output_dir / \"completions.json\"\n",
        "\n",
        "# Load existing completions if resuming\n",
        "if completions_file.exists():\n",
        "    print(f\"\\nLoading existing completions from {completions_file}...\")\n",
        "    try:\n",
        "        with open(completions_file, 'r') as f:\n",
        "            completions_by_model = json.load(f)\n",
        "        print(f\"Resumed: {len(completions_by_model.get(config.group_names[0], []))} completions for {config.group_names[0]}\")\n",
        "        print(f\"Resumed: {len(completions_by_model.get(config.group_names[1], []))} completions for {config.group_names[1]}\")\n",
        "    except:\n",
        "        print(\"Could not load existing completions, starting fresh\")\n",
        "        completions_by_model = {name: [] for name in config.group_names}\n",
        "\n",
        "# Generate completions\n",
        "for prompt_idx, prompt in enumerate(tqdm(APOLITICAL_PROMPTS, desc=\"Processing prompts\")):\n",
        "    for group_name in config.group_names:\n",
        "        model = models[group_name]\n",
        "        tokenizer = tokenizers[group_name]\n",
        "        \n",
        "        # Check if we already have enough completions for this prompt\n",
        "        current_count = len(completions_by_model.get(group_name, []))\n",
        "        if current_count >= (prompt_idx + 1) * config.num_completions:\n",
        "            continue  # Skip if already generated\n",
        "        \n",
        "        completions = generate_completions(\n",
        "            model, tokenizer, prompt,\n",
        "            num_completions=config.num_completions,\n",
        "            temperature=config.temperature,\n",
        "            top_p=config.top_p,\n",
        "            max_length=config.max_length,\n",
        "            seed=config.seed\n",
        "        )\n",
        "        \n",
        "        if group_name not in completions_by_model:\n",
        "            completions_by_model[group_name] = []\n",
        "        completions_by_model[group_name].extend(completions)\n",
        "        \n",
        "        # Save incrementally after each prompt\n",
        "        with open(completions_file, 'w') as f:\n",
        "            json.dump(completions_by_model, f, indent=2)\n",
        "\n",
        "print(f\"\\n Generated {len(completions_by_model[config.group_names[0]])} completions for {config.group_names[0]}\")\n",
        "print(f\" Generated {len(completions_by_model[config.group_names[1]])} completions for {config.group_names[1]}\")\n",
        "print(f\" Completions saved to {completions_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Extract Features\n",
        "# ============================================================================\n",
        "from collections import defaultdict\n",
        "\n",
        "def extract_lexical_features(text):\n",
        "    \"\"\"Extract lexical features from text.\"\"\"\n",
        "    if not text:\n",
        "        return {}\n",
        "    \n",
        "    words = text.split()\n",
        "    chars = text.replace(' ', '')\n",
        "    \n",
        "    features = {\n",
        "        'avg_word_length': np.mean([len(w) for w in words]) if words else 0,\n",
        "        'avg_sentence_length': np.mean([len(s.split()) for s in text.split('.') if s.strip()]) if text.split('.') else 0,\n",
        "        'vocab_diversity': len(set(words)) / len(words) if words else 0,  # Type-token ratio\n",
        "        'char_count': len(chars),\n",
        "        'word_count': len(words),\n",
        "        'sentence_count': len([s for s in text.split('.') if s.strip()]),\n",
        "        'uppercase_ratio': sum(1 for c in text if c.isupper()) / len(text) if text else 0,\n",
        "        'digit_ratio': sum(1 for c in text if c.isdigit()) / len(text) if text else 0,\n",
        "        'punctuation_ratio': sum(1 for c in text if c in '.,!?;:') / len(text) if text else 0,\n",
        "    }\n",
        "    \n",
        "    return features\n",
        "\n",
        "def extract_syntactic_features(text):\n",
        "    \"\"\"Extract syntactic features from text.\"\"\"\n",
        "    if not text:\n",
        "        return {}\n",
        "    \n",
        "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
        "    \n",
        "    features = {\n",
        "        'avg_sentence_length_chars': np.mean([len(s) for s in sentences]) if sentences else 0,\n",
        "        'max_sentence_length': max([len(s.split()) for s in sentences]) if sentences else 0,\n",
        "        'min_sentence_length': min([len(s.split()) for s in sentences]) if sentences else 0,\n",
        "        'sentence_length_std': np.std([len(s.split()) for s in sentences]) if sentences else 0,\n",
        "        'comma_count': text.count(','),\n",
        "        'question_marks': text.count('?'),\n",
        "        'exclamation_marks': text.count('!'),\n",
        "        'colon_count': text.count(':'),\n",
        "        'semicolon_count': text.count(';'),\n",
        "    }\n",
        "    \n",
        "    return features\n",
        "\n",
        "def extract_stylistic_features(text):\n",
        "    \"\"\"Extract stylistic features from text.\"\"\"\n",
        "    if not text:\n",
        "        return {}\n",
        "    \n",
        "    words = text.lower().split()\n",
        "    \n",
        "    # Function words (common words that don't carry much semantic meaning)\n",
        "    function_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', \n",
        "                      'of', 'with', 'by', 'from', 'as', 'is', 'are', 'was', 'were', 'be', \n",
        "                      'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', \n",
        "                      'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those'}\n",
        "    \n",
        "    function_word_count = sum(1 for w in words if w in function_words)\n",
        "    function_word_ratio = function_word_count / len(words) if words else 0\n",
        "    \n",
        "    # Hedging language (uncertainty markers)\n",
        "    hedging_words = {'maybe', 'perhaps', 'might', 'could', 'possibly', 'probably', \n",
        "                     'seems', 'appears', 'suggests', 'indicates', 'likely', 'unlikely'}\n",
        "    hedging_count = sum(1 for w in words if w in hedging_words)\n",
        "    hedging_ratio = hedging_count / len(words) if words else 0\n",
        "    \n",
        "    # Contractions\n",
        "    contractions = [\"'t\", \"'s\", \"'re\", \"'ve\", \"'ll\", \"'d\", \"n't\"]\n",
        "    contraction_count = sum(text.count(c) for c in contractions)\n",
        "    \n",
        "    # First person markers\n",
        "    first_person = {'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n",
        "    first_person_count = sum(1 for w in words if w in first_person)\n",
        "    first_person_ratio = first_person_count / len(words) if words else 0\n",
        "    \n",
        "    features = {\n",
        "        'function_word_ratio': function_word_ratio,\n",
        "        'hedging_ratio': hedging_ratio,\n",
        "        'contraction_count': contraction_count,\n",
        "        'first_person_ratio': first_person_ratio,\n",
        "    }\n",
        "    \n",
        "    return features\n",
        "\n",
        "def extract_all_features(text):\n",
        "    \"\"\"Extract all stylistic features from text.\"\"\"\n",
        "    lexical = extract_lexical_features(text)\n",
        "    syntactic = extract_syntactic_features(text)\n",
        "    stylistic = extract_stylistic_features(text)\n",
        "    \n",
        "    return {**lexical, **syntactic, **stylistic}\n",
        "\n",
        "def generate_feature_matrix(completions_by_model):\n",
        "    \"\"\"\n",
        "    Generate feature matrix for all completions.\n",
        "    \n",
        "    Returns:\n",
        "        X: Feature matrix (n_samples, n_features)\n",
        "        y: Labels (0 for first model, 1 for second model)\n",
        "        feature_names: List of feature names\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "    feature_names = None\n",
        "    \n",
        "    model_names = list(completions_by_model.keys())\n",
        "    \n",
        "    for model_idx, (model_name, completions) in enumerate(completions_by_model.items()):\n",
        "        for completion in completions:\n",
        "            features = extract_all_features(completion)\n",
        "            \n",
        "            if feature_names is None:\n",
        "                feature_names = sorted(features.keys())\n",
        "            \n",
        "            feature_vector = [features.get(fname, 0) for fname in feature_names]\n",
        "            X.append(feature_vector)\n",
        "            y.append(model_idx)\n",
        "    \n",
        "    return np.array(X), np.array(y), feature_names\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXTRACTING STYLISTIC FEATURES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "X, y, feature_names = generate_feature_matrix(completions_by_model)\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Number of features: {len(feature_names)}\")\n",
        "print(f\"Features: {', '.join(feature_names)}\")\n",
        "\n",
        "# Split features by model\n",
        "us_mask = (y == 0)\n",
        "uk_mask = (y == 1)\n",
        "features_us = X[us_mask]\n",
        "features_uk = X[uk_mask]\n",
        "\n",
        "# Save raw features for later plot generation\n",
        "features_file = output_dir / \"raw_features.npz\"\n",
        "np.savez_compressed(\n",
        "    features_file,\n",
        "    X=X,\n",
        "    y=y,\n",
        "    features_us=features_us,\n",
        "    features_uk=features_uk,\n",
        "    feature_names=np.array(feature_names)\n",
        ")\n",
        "print(f\"\\n Raw features saved to {features_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Hypothesis 1 Analysis (JS Divergence)\n",
        "# ============================================================================\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "def compute_jensen_shannon_divergence(features_us, features_uk, feature_names):\n",
        "    \"\"\"\n",
        "    Compute Jensen-Shannon divergence between US and UK feature distributions.\n",
        "    \"\"\"\n",
        "    js_divergences = {}\n",
        "    \n",
        "    for i, feature_name in enumerate(feature_names):\n",
        "        us_values = features_us[:, i]\n",
        "        uk_values = features_uk[:, i]\n",
        "        \n",
        "        # Create histograms (normalize to get probability distributions)\n",
        "        # Use same bins for both distributions\n",
        "        all_values = np.concatenate([us_values, uk_values])\n",
        "        bins = np.linspace(np.min(all_values), np.max(all_values), 50)\n",
        "        \n",
        "        us_hist, _ = np.histogram(us_values, bins=bins, density=True)\n",
        "        uk_hist, _ = np.histogram(uk_values, bins=bins, density=True)\n",
        "        \n",
        "        # Normalize\n",
        "        us_hist = us_hist / (us_hist.sum() + 1e-10)\n",
        "        uk_hist = uk_hist / (uk_hist.sum() + 1e-10)\n",
        "        \n",
        "        # Compute JS divergence\n",
        "        js_div = jensenshannon(us_hist, uk_hist)\n",
        "        js_divergences[feature_name] = float(js_div)\n",
        "    \n",
        "    return js_divergences\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HYPOTHESIS 1: JENSEN-SHANNON DIVERGENCE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "js_divergences = compute_jensen_shannon_divergence(features_us, features_uk, feature_names)\n",
        "\n",
        "sorted_js = sorted(js_divergences.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"\\nTop features by JS divergence:\")\n",
        "for feature_name, js_div in sorted_js[:10]:\n",
        "    print(f\"  {feature_name}: {js_div:.4f}\")\n",
        "\n",
        "# Overall JS divergence (average)\n",
        "overall_js = np.mean(list(js_divergences.values()))\n",
        "print(f\"\\nOverall average JS divergence: {overall_js:.4f}\")\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    'hypothesis': 'H1',\n",
        "    'jensen_shannon_divergences': js_divergences,\n",
        "    'overall_js_divergence': float(overall_js),\n",
        "    'feature_names': feature_names,\n",
        "}\n",
        "\n",
        "results_path = output_dir / \"h1_results.json\"\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\n Results saved to {results_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Generate H1 Visualizations\n",
        "# ============================================================================\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_js_divergence(js_divergences, overall_js, output_path):\n",
        "    \"\"\"Plot Jensen-Shannon divergence per feature (H1).\"\"\"\n",
        "    # Sort by JS divergence\n",
        "    sorted_js = sorted(js_divergences.items(), key=lambda x: x[1], reverse=True)\n",
        "    features, values = zip(*sorted_js)\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    bars = plt.barh(range(len(features)), values, color='steelblue', alpha=0.7)\n",
        "    \n",
        "    # Add overall average line\n",
        "    plt.axvline(overall_js, color='red', linestyle='--', linewidth=2, \n",
        "                label=f'Overall Average: {overall_js:.3f}')\n",
        "    \n",
        "    plt.yticks(range(len(features)), features)\n",
        "    plt.xlabel('Jensen-Shannon Divergence', fontsize=12)\n",
        "    plt.title('H1: Distributional Divergence by Feature', fontsize=14, fontweight='bold')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3, axis='x')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"JS divergence plot saved to {output_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING H1 VISUALIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# H1: JS Divergence plot\n",
        "js_plot_path = output_dir / \"h1_js_divergence.png\"\n",
        "plot_js_divergence(js_divergences, overall_js, js_plot_path)\n",
        "\n",
        "print(\"\\n All H1 visualizations generated!\")\n",
        "print(f\" Plots saved to {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 10: Cleanup\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLEANUP\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Clear models from memory\n",
        "for model in models.values():\n",
        "    del model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Models cleared from memory\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HYPOTHESIS 1 EVALUATION COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nResults saved to: {output_dir}\")\n",
        "print(f\"  - Completions: {completions_file}\")\n",
        "print(f\"  - Raw features: {features_file}\")\n",
        "print(f\"  - Results: {results_path}\")\n",
        "print(f\"  - Visualizations: {js_plot_path}\")\n",
        "print(f\"\\nOverall JS divergence: {overall_js:.4f}\")\n",
        "print(f\"\\nNext: Run H3 notebook for calibration analysis\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
